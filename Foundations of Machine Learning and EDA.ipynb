{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b5ff9-f7bc-4ad9-9f0c-9baaa1b965f4",
   "metadata": {},
   "source": [
    "# Question 1 :  What is the difference between AI, ML, DL, and Data Science? Provide a brief explanation of each. (Hint: Compare their scope, techniques, and applications for each.) \n",
    "Answer:  Differences between AI, ML, DL, and data science\n",
    "\n",
    " #Scope:\n",
    "- AI: Broad field aiming to build systems that perform tasks requiring human-like intelligence (reasoning, perception, decision-making).\n",
    "- ML: Subfield of AI focused on learning patterns from data to make predictions/decisions with minimal explicit programming.\n",
    "- DL: Subfield of ML using multi-layer neural networks to learn hierarchical representations, especially in high-dimensional data.\n",
    "- Data science: Interdisciplinary field combining statistics, ML, programming, and domain knowledge to extract insights and support decisions.\n",
    "#Techniques:\n",
    "- AI: Planning, search (A*), logic, knowledge graphs, expert systems, ML methods.\n",
    "- ML: Supervised/unsupervised learning (regression, decision trees, SVM, clustering), model evaluation.\n",
    "- DL: CNNs, RNNs/LSTMs, Transformers, autoencoders, optimization via backpropagation.\n",
    "- Data science: Data wrangling, EDA, statistical inference, ML modeling, visualization, experimentation.\n",
    "#Applications:\n",
    "- AI: Robotics, autonomous agents, dialog systems, game-playing.\n",
    "- ML: Credit scoring, recommendation systems, demand forecasting.\n",
    "- DL: Image recognition, speech/NLP, generative models.\n",
    "- Data science: Business analytics, A/B testing, dashboards, operational decision support.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1097f-a4b2-4964-9b9d-d09276e3f155",
   "metadata": {},
   "source": [
    "# Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent them? Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques. \n",
    "Answer:  \n",
    "Overfitting and underfitting in ML\n",
    "## Definitions:\n",
    "- Overfitting: Model captures noise and idiosyncrasies of the training data, leading to low training error but high test error.\n",
    "- Underfitting: Model is too simple to capture underlying patterns, leading to high error on both training and test data.\n",
    "#Bias-variance tradeoff:\n",
    "- High variance (overfitting): Complex models sensitive to training fluctuations.\n",
    "- High bias (underfitting): Simple models that miss important structure.\n",
    "- Goal is to minimize total error by balancing bias and variance.\n",
    "#Detection:\n",
    "- Train vs. validation curves: Large generalization gap signals overfitting; high errors on both sets indicate underfitting.\n",
    "- Learning curves: If adding data reduces validation error (variance problem), or error plateaus high (bias problem).\n",
    "- Cross-validation: Consistent performance across folds vs. variability indicates stability or overfitting risks.\n",
    "#Prevention:\n",
    "- Regularization: L1/L2 penalties, dropout (DL), data augmentation, early stopping.\n",
    "- Model complexity control: Pruning trees, limiting polynomial degree, simplifying architectures.\n",
    "- Cross-validation & hyperparameter tuning: Grid/random/Bayesian search with k-fold CV.\n",
    "- More/better data: Increase sample size, improve data quality, feature engineering.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38328d8f-3400-4010-a672-eab47850f940",
   "metadata": {},
   "source": [
    "# Question 3:How would you handle missing values in a dataset? Explain at least three methods with examples. Hint: Consider deletion, mean/median imputation, and predictive modeling. \n",
    "Answer: Handling missing values in a dataset\n",
    "\n",
    "#Deletion:\n",
    "- Listwise deletion: Remove rows with missing values; suitable when missingness is rare and MCAR (missing completely at random).\n",
    "- Example: If 1% rows have missing Age, drop them in a large dataset to avoid bias.\n",
    "- Risk: Loss of data; biased if missingness relates to outcome or features.\n",
    "#Simple imputation:\n",
    "- Mean/median (numeric): Median for skewed distributions; mean for roughly symmetric.\n",
    "- Mode (categorical): Replace with most frequent category.\n",
    "- Example: Replace missing income with median income; replace missing City with mode “Mumbai.”\n",
    "- Risk: Underestimates variability and can bias relationships.\n",
    "#Predictive modeling:\n",
    "- Impute via models: Train a model (e.g., regression, k-NN, iterative imputer) to predict missing values from other features.\n",
    "- Example: Predict missing Age using Fare, Pclass, Sex in Titanic via iterative imputer.\n",
    "- Pros/cons: More accurate; adds model uncertainty; must avoid target leakage by fitting on training data only.\n",
    "#Best practices:\n",
    "- Diagnose mechanism: MCAR vs. MAR vs. MNAR.\n",
    "- Flag imputed values: Add indicator features to capture missingness patterns.\n",
    "- Evaluate impact: Compare models with/without imputation strategies.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376022bf-7fe7-46a6-a417-415ae0b7e5aa",
   "metadata": {},
   "source": [
    "# Question 4:What is an imbalanced dataset? Describe two techniques to handle it (theoretical + practical). Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models. \n",
    "Answer:Imbalanced datasets and handling techniques\n",
    "## Definition:\n",
    "- Class distribution is skewed (e.g., 95% negatives, 5% positives), causing models to favor the majority class and misleading metrics like accuracy.\n",
    "#Theoretical techniques:\n",
    "- Class weights: Penalize misclassification of minority class more during training (supported in many algorithms like logistic regression, tree-based models, SVM, neural nets).\n",
    "- SMOTE: Synthetic Minority Over-sampling Technique creates synthetic minority samples by interpolating between nearest neighbors.\n",
    "#Practical techniques:\n",
    "- Random oversampling/undersampling: Duplicate minority samples or remove majority samples to balance classes; quick and effective but risks overfitting (oversampling) or losing information (undersampling).\n",
    "- Pipeline considerations: Apply resampling on the training split only; use stratified CV; evaluate with precision-recall, AUROC, F1 instead of accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff19507-c602-4a25-8cf6-f3934af8af06",
   "metadata": {},
   "source": [
    "# Question 5: Why is feature scaling important in ML? Compare Min-Max scaling and Standardization. Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient descent. \n",
    "Answer:-\n",
    "## Importance:\n",
    "- Distance-based algorithms (KNN, k-means), margin-based (SVM with RBF), and gradient descent optimization are sensitive to feature scales; unscaled     features can dominate distances/margins and slow or destabilize optimization.\n",
    "- Tree-based methods (Random Forest, XGBoost) are largely scale-invariant.\n",
    "#Min-Max scaling (normalization):\n",
    "- Transform: x'=\\frac{x-\\min }{\\max -\\min } to range [0, 1] (or another specified range).\n",
    "- Pros: Preserves original distribution and relative distances; bounded outputs beneficial for algorithms assuming limited ranges or image pixels.\n",
    "- Cons: Sensitive to outliers; min/max shifts with new data; not centering.\n",
    "#Standardization (z-score):\n",
    "- Transform: x'=\\frac{x-\\mu }{\\sigma }, mean 0, variance 1.\n",
    "- Pros: More robust to outliers than min-max; suits models assuming Gaussianity; improves convergence in linear/logistic regression and neural nets.\n",
    "- Cons: Unbounded; interpretation less intuitive; still affected by extreme outliers (use robust scalers if needed).\n",
    "#When to use:\n",
    "- Min-Max: KNN, distance-based methods where bounded scales help; neural nets with bounded activations; data without heavy outliers.\n",
    "- Standardization: SVM, linear models, PCA, logistic regression, deep learning; mixed-scale features; presence of outliers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e45a06-92f1-4bf6-9f74-2696055a0775",
   "metadata": {},
   "source": [
    "# Question 6:  Compare Label Encoding and One-Hot Encoding. When would you prefer one over the other? Hint: Consider categorical variables with ordinal vs. nominal relationships.\n",
    "Answer:-\n",
    "\n",
    "## Label encoding vs. one-hot encoding\n",
    "### Label encoding:\n",
    "- What: Map categories to integers (A→0, B→1, C→2).\n",
    "- Use when: Ordinal categorical variables where order matters (e.g., size: Small < Medium < Large).\n",
    "- Risk: Creates false ordinal relationships for nominal variables; tree models can sometimes tolerate it, but linear/distance-based models               mayMisinterpret magnitude.\n",
    "### One-hot encoding:\n",
    "- What: Create binary indicator columns per category.\n",
    "- Use when: Nominal variables with no inherent order (e.g., city names, colors).\n",
    "- Trade-offs: Increases dimensionality; can be sparse; necessary for models that interpret values as distances or weights.\n",
    "- Guidance:\n",
    "- Prefer label encoding for ordinal features and one-hot for nominal. For high-cardinality nominal features, consider target encoding, hashing, or embeddings (with careful validation to avoid leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41bb4e1-0e39-4e74-9996-4a74eda17fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 categories by average rating:\n",
      "Category\n",
      "1.9                    19.000000\n",
      "EVENTS                  4.435556\n",
      "EDUCATION               4.389032\n",
      "ART_AND_DESIGN          4.358065\n",
      "BOOKS_AND_REFERENCE     4.346067\n",
      "PERSONALIZATION         4.335987\n",
      "PARENTING               4.300000\n",
      "GAME                    4.286326\n",
      "BEAUTY                  4.278571\n",
      "HEALTH_AND_FITNESS      4.277104\n",
      "\n",
      "Bottom 10 categories by average rating:\n",
      "Category\n",
      "NEWS_AND_MAGAZINES     4.132189\n",
      "FINANCE                4.131889\n",
      "ENTERTAINMENT          4.126174\n",
      "BUSINESS               4.121452\n",
      "TRAVEL_AND_LOCAL       4.109292\n",
      "LIFESTYLE              4.094904\n",
      "VIDEO_PLAYERS          4.063750\n",
      "MAPS_AND_NAVIGATION    4.051613\n",
      "TOOLS                  4.047411\n",
      "DATING                 3.970769\n",
      "\n",
      "App counts per category (top 10):\n",
      "Category\n",
      "FAMILY             1747\n",
      "GAME               1097\n",
      "TOOLS               734\n",
      "PRODUCTIVITY        351\n",
      "MEDICAL             350\n",
      "COMMUNICATION       328\n",
      "FINANCE             323\n",
      "SPORTS              319\n",
      "PHOTOGRAPHY         317\n",
      "PERSONALIZATION     314\n"
     ]
    }
   ],
   "source": [
    "# Question 7:  Google Play Store Dataset a). Analyze the relationship between app categories and ratings. Which categories have the highest/lowest average ratings, and what could be the possible reasons? Dataset: https://github.com/MasteriNeuron/datasets.git (Include your Python code and output in the code box below.) \n",
    "\n",
    "# Answer: \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/googleplaystore.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df[df['Rating'].notna()]\n",
    "df = df[df['Category'].notna()]\n",
    "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
    "df = df.dropna(subset=['Rating'])\n",
    "\n",
    "# Aggregate average ratings by category\n",
    "cat_ratings = df.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Top and bottom 10 categories by average rating\n",
    "top10 = cat_ratings.head(10)\n",
    "bottom10 = cat_ratings.tail(10)\n",
    "\n",
    "print(\"Top 10 categories by average rating:\")\n",
    "print(top10.to_string())\n",
    "print(\"\\nBottom 10 categories by average rating:\")\n",
    "print(bottom10.to_string())\n",
    "\n",
    "# Optional: count apps per category (to contextualize averages)\n",
    "counts = df['Category'].value_counts()\n",
    "\n",
    "print(\"\\nApp counts per category (top 10):\")\n",
    "print(counts.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efd255f-decf-4ac8-b384-d52f87a13d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate by Pclass:\n",
      "Pclass\n",
      "1    62.96%\n",
      "2    47.28%\n",
      "3    24.24%\n",
      "Name: Survived, dtype: object\n",
      "\n",
      "Survival rate by AgeGroup:\n",
      "AgeGroup\n",
      "Child    50.36%\n",
      "Adult    38.26%\n",
      "Name: Survived, dtype: object\n",
      "\n",
      "Survival rate by Pclass and AgeGroup (%):\n",
      "AgeGroup  Child  Adult\n",
      "Pclass                \n",
      "1         87.50  63.53\n",
      "2         79.31  41.67\n",
      "3         35.11  19.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13520\\70032130.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  age_survival = df.groupby('AgeGroup')['Survived'].mean()\n",
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_13520\\70032130.py:30: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index='Pclass', columns='AgeGroup', values='Survived', aggfunc='mean')\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Titanic Dataset \n",
    "#a) Compare the survival rates based on passenger class (Pclass). Which class had the highest survival rate, and why do you think that happened? \n",
    "#b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and adults (Age ≥ 18). Did children have a better chance of survival? \n",
    "#Dataset: https://github.com/MasteriNeuron/datasets.git (Include your Python code and output in the code box below.) \n",
    "\n",
    "# Answer: \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Ensure columns\n",
    "df = df[['Survived','Pclass','Age','Sex']]\n",
    "\n",
    "# Survival rates by Pclass\n",
    "pclass_survival = df.groupby('Pclass')['Survived'].mean().sort_index()\n",
    "\n",
    "# Age groups: children (<18) vs adults (>=18)\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=[-float('inf'), 18, float('inf')], labels=['Child','Adult'])\n",
    "age_survival = df.groupby('AgeGroup')['Survived'].mean()\n",
    "\n",
    "print(\"Survival rate by Pclass:\")\n",
    "print((pclass_survival*100).round(2).astype(str) + \"%\")\n",
    "\n",
    "print(\"\\nSurvival rate by AgeGroup:\")\n",
    "print((age_survival*100).round(2).astype(str) + \"%\")\n",
    "\n",
    "# Additional: cross-tab by class and age group\n",
    "pivot = df.pivot_table(index='Pclass', columns='AgeGroup', values='Survived', aggfunc='mean')\n",
    "print(\"\\nSurvival rate by Pclass and AgeGroup (%):\")\n",
    "print((pivot*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3821f9c-cebe-4146-98d6-d9ada425d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price by days left (first 15):\n",
      "days_left\n",
      "1     21591.87\n",
      "2     30211.30\n",
      "3     28976.08\n",
      "4     25730.91\n",
      "5     26679.77\n",
      "6     24856.49\n",
      "7     25588.37\n",
      "8     24895.88\n",
      "9     25726.25\n",
      "10    25572.82\n",
      "11    22990.66\n",
      "12    22505.80\n",
      "13    22498.89\n",
      "14    22678.00\n",
      "15    21952.54\n",
      "\n",
      "Detected surge days (heuristic): [2]\n",
      "Suggested lowest-price window center day_left: 49\n",
      "\n",
      "Delhi-Mumbai average prices by airline:\n",
      "airline\n",
      "AirAsia       3981.19\n",
      "Indigo        4473.74\n",
      "SpiceJet      4628.25\n",
      "GO_FIRST      5762.21\n",
      "Air_India    23695.92\n",
      "Vistara      26630.29\n",
      "\n",
      "Relative classification:\n",
      "airline\n",
      "AirAsia      Cheaper\n",
      "Indigo       Cheaper\n",
      "SpiceJet     Cheaper\n",
      "GO_FIRST     Cheaper\n",
      "Air_India    Premium\n",
      "Vistara      Premium\n"
     ]
    }
   ],
   "source": [
    "#Question 9: Flight Price Prediction Dataset \n",
    "#a) How do flight prices vary with the days left until departure? Identify any exponential price surges and recommend the best booking window. \n",
    "#b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are consistently cheaper/premium, and why? \n",
    "#Dataset: https://github.com/MasteriNeuron/datasets.git (Include your Python code and output in the code box below.)\n",
    "\n",
    "#Answer:  \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/main/flight_price.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Standardize column names based on common schemas\n",
    "# Assume columns: 'price','airline','source','destination','days_left','route'\n",
    "# If 'route' absent, create from source-destination\n",
    "if 'route' not in df.columns and {'source_city','destination_city'}.issubset(df.columns):\n",
    "    df['route'] = df['source_city'].str.strip() + \"-\" + df['destination_city'].str.strip()\n",
    "\n",
    "# Clean\n",
    "df = df.dropna(subset=['price','days_left'])\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df['days_left'] = pd.to_numeric(df['days_left'], errors='coerce')\n",
    "df = df.dropna(subset=['price','days_left'])\n",
    "\n",
    "# a) Price vs days_left\n",
    "days_price = df.groupby('days_left')['price'].mean().sort_index()\n",
    "\n",
    "# Identify surge window: compute relative change\n",
    "rel_change = days_price.pct_change().fillna(0)\n",
    "# Example heuristic: surge when relative change > 0.15 day-over-day for close-in dates\n",
    "surge_days = rel_change[rel_change > 0.15].index.tolist()\n",
    "\n",
    "print(\"Average price by days left (first 15):\")\n",
    "print(days_price.head(15).round(2).to_string())\n",
    "\n",
    "print(\"\\nDetected surge days (heuristic):\", surge_days)\n",
    "\n",
    "# Recommend booking window: look for plateau/minima\n",
    "rolling = days_price.rolling(7, min_periods=3).mean()\n",
    "min_window_day = rolling.idxmin()\n",
    "print(\"Suggested lowest-price window center day_left:\", int(min_window_day))\n",
    "\n",
    "# b) Airline comparison for Delhi-Mumbai\n",
    "route_mask = df['route'].str.lower() == 'delhi-mumbai'\n",
    "dm = df[route_mask]\n",
    "airline_avg = dm.groupby('airline')['price'].mean().sort_values()\n",
    "print(\"\\nDelhi-Mumbai average prices by airline:\")\n",
    "print(airline_avg.round(2).to_string())\n",
    "\n",
    "# Classify airlines as cheaper/premium relative to route average\n",
    "route_avg = dm['price'].mean()\n",
    "labels = (airline_avg / route_avg).apply(lambda x: 'Cheaper' if x < 0.95 else ('Premium' if x > 1.05 else 'Average'))\n",
    "print(\"\\nRelative classification:\")\n",
    "print(labels.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9692f5-10b6-4e54-b784-e72e595673b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
